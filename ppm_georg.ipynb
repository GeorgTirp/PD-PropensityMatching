{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "from icecream import ic\n",
    "from typing import Dict, Callable\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Base Data Class\n",
    "class Data:\n",
    "    def __init__(self, path_of_folder: str, foldertype: str = \"PPMI\"):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the given path and initializes the data structures.\n",
    "        \"\"\"\n",
    "        self.path = path_of_folder\n",
    "        self.foldertype = foldertype\n",
    "        if foldertype==\"PPMI\":\n",
    "            self.complete_data = self.load_ppmi(path_of_folder, foldertype)\n",
    "        elif foldertype==\"tuebingen\":\n",
    "            self.complete_data = self.load_tuebingen(path_of_folder, foldertype)\n",
    "        elif foldertype==\"custom\":\n",
    "            self.complete_data = self.load_costum(path_of_folder, foldertype)\n",
    "        else:\n",
    "            raise ValueError(\"For Folder, only 'PPMI', 'tuebingen' or 'custom' are allowed\")\n",
    "\n",
    "        self.covariates = None  # Subset of data used for matching\n",
    "        self.covariates_longitude = None  # Data for longitudinal analysis\n",
    "        self.df = None  # Data in format for statistical models probably numpy or something similar\n",
    "        self.is_converted_to_standard = False\n",
    "\n",
    "    def load_ppmi(self, path_of_folder: str, folder: str):\n",
    "        \"\"\"\n",
    "        Loads in data according to PPMI layout\n",
    "        \"\"\"\n",
    "        if folder!=\"PPMI\":\n",
    "            raise ValueError(\"For Folder, only 'PPMI' is allowed\")\n",
    "        \n",
    "        datdir =path_of_folder\n",
    "\n",
    "\n",
    "        stat = pd.read_csv(datdir+'/Participant_Status_13Feb2024.csv')\n",
    "        dbs = pd.read_csv(datdir+'/Surgery_for_PD_Log_15Feb2024.csv')\n",
    "        dbs = dbs.loc[dbs.PDSURGTP==1,:]\n",
    "        medication = pd.read_csv(datdir+'/LEDD_Concomitant_Medication_Log_24Apr2024.csv')\n",
    "        \n",
    "        diaghist = pd.read_csv(datdir+'/PD_Diagnosis_History_21Feb2024.csv')\n",
    "        diaghist.SXDT = pd.to_datetime(diaghist.SXDT)\n",
    "\n",
    "        demo = pd.read_csv(datdir+'/Demographics_25Apr2024.csv')\n",
    "        demo['SEX'] = np.array(['female', 'male'])[demo['SEX']]\n",
    "        #%% updrs\n",
    "\n",
    "        #Load in and merge to full updrs1\n",
    "        updrs1 = pd.read_csv(datdir+'/MDS-UPDRS_Part_I_15Feb2024.csv')\n",
    "        updrs1p = pd.read_csv(datdir+'/MDS-UPDRS_Part_I_Patient_Questionnaire_15Feb2024.csv')\n",
    "        updrs1 = pd.merge(updrs1, updrs1p, on='PATNO', how='outer')\n",
    "\n",
    "        #Load in and merge to full updrs2\n",
    "        updrs2p = pd.read_csv(datdir+'/MDS_UPDRS_Part_II__Patient_Questionnaire_15Feb2024.csv')\n",
    "        \n",
    "        updrs3 = pd.read_csv(datdir+'/MDS-UPDRS_Part_III_15Feb2024.csv')\n",
    "\n",
    "\n",
    "        UPDRSrig = [\"NP3RIGLL\",\n",
    "                    \"NP3RIGLU\",\n",
    "                    \"NP3RIGN\",\n",
    "                    \"NP3RIGRL\",\n",
    "                    \"NP3RIGRU\"]\n",
    "\n",
    "        updrs3['rigidity'] = updrs3.loc[:,UPDRSrig].sum(1)\n",
    "\n",
    "        updrs3[\"bradykinesia\"] = updrs3.loc[:,\"NP3BRADY\"]\n",
    "        UPDRSlat = [\"NP3FTAP\",\n",
    "                    \"NP3HMOV\",\n",
    "                    \"NP3KTRM\",\n",
    "                    \"NP3LGAG\", \n",
    "                     \"NP3PRSP\",\n",
    "                     \"NP3PTRM\"]\n",
    "\n",
    "        updrs3['latindex'] = (updrs3.loc[:,[i+\"R\" for i in UPDRSlat]].sum(1) - updrs3.loc[:,[i+\"L\" for i in UPDRSlat]].sum(1))#/updrs3.loc[:,[i+\"R\" for i in UPDRSlat]].sum(1) + updrs3.loc[:,[i+\"L\" for i in UPDRSlat]].sum(1)\n",
    "        latix = updrs3.groupby([\"PATNO\",\"EVENT_ID\", \"PDSTATE\"],as_index=False).latindex.mean()\n",
    "        ##average over MED ON and OFF states (only for UPDRS3)\n",
    "        #updrs3 = updrs3.groupby([\"PATNO\",\"EVENT_ID\"],as_index=False).NP3TOT.mean()\n",
    "        updrs3 = pd.merge(updrs3,latix)\n",
    "\n",
    "        updrs4 = pd.read_csv(datdir+'/MDS-UPDRS_Part_IV__Motor_Complications_15Feb2024.csv')\n",
    "        #updrs4.NP4TOT\n",
    "        mds_updrs = dict(mds_updrs1=updrs1, mds_updrs2=updrs2p, mds_updrs3=updrs3, mds_updrs4=updrs4)\n",
    "\n",
    "        #updrs.loc[:,\"UPDRS_SUMSCORE\"] = updrs.loc[:,['NP1RTOT','NP1PTOT','NP2PTOT','NP3TOT','NP4TOT']].sum(1)\n",
    "\n",
    "        moca = pd.read_csv(datdir+'/Montreal_Cognitive_Assessment__MoCA__12Mar2024.csv')\n",
    "\n",
    "        #REM sleep questionnaire\n",
    "        rbd = pd.read_csv(datdir+'/REM_Sleep_Behavior_Disorder_Questionnaire_08Feb2024.csv')\n",
    "        \n",
    "        ppmi_dict = {\"stat\":stat, \"dbs\":dbs, \"medication\": medication, \"diaghist\":diaghist, \"demo\":demo, \"mds_updrs\":mds_updrs, \"moca\":moca, \"rbd\":rbd}\n",
    "\n",
    "        return ppmi_dict\n",
    "\n",
    "    def load_from_csv(self, csv_path, col_dict):\n",
    "        df = pd.read_csv(csv_path, usecols=col_dict.keys())\n",
    "        df = df.rename(columns=col_dict)\n",
    "        return df\n",
    "\n",
    "    def load_tuebingen(self, path_of_folder: str, foldertype: str):\n",
    "        \"\"\"\n",
    "        Loads in data from costom (your) data folder. Has to be implemented according to your data layout\n",
    "        \"\"\"\n",
    "        if foldertype!=\"tuebingen\":\n",
    "            raise ValueError(\"For Folder, only 'tuebingen' is allowed\")\n",
    "        \n",
    "        datadir = path_of_folder\n",
    "        #Load in DBS Data\n",
    "        bdi = pd.read_excel(datadir + '/1. BDI/BDI_v1.xlsx')\n",
    "        bdi = pd.read_excel(datadir + '/1. BDI/BDI_v2-aktuell.xlsx')\n",
    "\n",
    "        #Load in Medication\n",
    "        medication = pd.read_excel(datadir + '/3. Medication/Medication.xlsx')\n",
    "\n",
    "        #Load in Demographics\n",
    "        demo = pd.read_excel(datadir + '/6. Demographics/PD_demographics.xlsx')\n",
    "\n",
    "        #Create unique identifier and map on Patient numbers that will get used in the other dataframes\n",
    "        demo = demo.dropna(subset=['OP_DATUM'])\n",
    "        demo['Unique identifier']  = demo['OP_DATUM'].astype(str) + \"_\" + demo['GEB_DATUM'].astype(str)\n",
    "        \n",
    "        identifiers = demo['Unique identifier'].tolist()\n",
    "        IDs = list(range(1, len((identifiers))+1))\n",
    "        ui_id_mapping = dict(zip(identifiers, IDs))\n",
    "        demo.insert(0, \"PATNO\",demo['Unique identifier'].map(ui_id_mapping))\n",
    "        demo = demo.drop('Unique identifier', axis=1)\n",
    "        #df = df.drop('Unique identifier', axis=1)\n",
    "        \n",
    "        #Function to add PATNO to the other dataframes according to the unique identifier in the demographics dataframe\n",
    "        def add_patno(df, mapping_dict=ui_id_mapping):\n",
    "            df = df.dropna(subset=['OP_DATUM'])\n",
    "            df['Unique identifier'] = df['OP_DATUM'].astype(str) + \"_\" + df['GEB_DATUM'].astype(str)\n",
    "            df.insert(0, \"PATNO\", df['Unique identifier'].map(mapping_dict))\n",
    "            nan_patno = df['PATNO'].isna()\n",
    "            if nan_patno.any():\n",
    "                new_identifiers = df.loc[nan_patno, 'Unique identifier'].tolist()\n",
    "                new_ids = list(range(max(mapping_dict.values()) + 1, max(mapping_dict.values()) + 1 + len(new_identifiers)))\n",
    "                new_mapping = dict(zip(new_identifiers, new_ids))\n",
    "                mapping_dict.update(new_mapping)\n",
    "                df.loc[nan_patno, 'PATNO'] = df.loc[nan_patno, 'Unique identifier'].map(new_mapping)\n",
    "            df['PATNO'] = df['PATNO'].astype(int)\n",
    "            df = df.drop('Unique identifier', axis=1)\n",
    "            return df    \n",
    "        #demo = demo.rename(columns={\"Unique identifier\": \"PATNO\"})\n",
    "\n",
    "        #Load in MDS-UPDRS\n",
    "        mds_updrs1 = add_patno(pd.read_excel(datadir + '/2. MDS-UPDRS/MDS-UPDRS_I.xlsx'))\n",
    "        mds_updrs2 = add_patno(pd.read_excel(datadir + '/2. MDS-UPDRS/MDS-UPDRS_II.xlsx'))\n",
    "        mds_updrs3 = add_patno(pd.read_excel(datadir + '/2. MDS-UPDRS/MDS-UPDRS_III.xlsx'))\n",
    "        mds_updrs4 = add_patno(pd.read_excel(datadir + '/2. MDS-UPDRS/MDS-UPDRS_IV.xlsx'))\n",
    "        \n",
    "        #Add a total score for each updrs part\n",
    "        mds_updrs1['MDS-UPDRS_1.TOT'] = mds_updrs1.iloc[:, 5:].sum(axis=1)\n",
    "        mds_updrs2['MDS-UPDRS_2.TOT'] = mds_updrs2.iloc[:, 5:].sum(axis=1)\n",
    "        #mds_updrs3['MDS-UPDRS_1.TOT'] = mds_updrs3.iloc[:, 5:].sum(axis=1)\n",
    "        mds_updrs4['MDS-UPDRS_4.TOT'] = mds_updrs4.iloc[:, 5:].sum(axis=1)\n",
    "        mds_updrs = dict(mds_updrs1=mds_updrs1, mds_updrs2=mds_updrs2, mds_updrs3=mds_updrs3, mds_updrs4=mds_updrs4)\n",
    "        \n",
    "        #Load in MoCA\n",
    "        moca = add_patno(pd.read_excel(datadir + '/4. MoCA/MoCA.xlsx'))\n",
    "\n",
    "        #Load in PDQ-39\n",
    "        pdq39 = add_patno(pd.read_excel(datadir + '/5. PDQ-39/PDQ-39.xlsx'))\n",
    "\n",
    "        #Load in Stimulation\n",
    "        stimulation = add_patno(pd.read_excel(datadir + '/7. Stimulation/Stimulation_parameters.xlsx'))\n",
    "\n",
    "        #Load in BBS\n",
    "        bbs = add_patno(pd.read_excel(datadir + '/8. BBS/BBS.xlsx'))\n",
    "\n",
    "        #Load in UPDRS (OLD)\n",
    "        updrs_old = add_patno(pd.read_excel(datadir + '/9. UPDRS (OLD)/UPDRS_III.xlsx'))\n",
    "\n",
    "        \n",
    "\n",
    "        dbs_dict = {\"bdi\":bdi, \"medication\":medication, \"demo\":demo, \"mds_updrs\":mds_updrs, \"moca\":moca, \"pdq39\":pdq39, \"stimulation\":stimulation, \"bbs\":bbs, \"updrs_old\":updrs_old}\n",
    "        if foldertype==\"tuebingen\":\n",
    "            self.export_covariate_names(dbs_dict, path_of_folder)\n",
    "        return dbs_dict\n",
    "    \n",
    "\n",
    "    def load_costum(self, path_of_folder: str, foldertype: str):\n",
    "        \"\"\"\n",
    "        Loads in data from costom (your) data folder. Has to be implemented according to your data layout\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def export_covariate_names(self, full_df, path: str):\n",
    "        \"\"\"\n",
    "        Assuming that this dataset will be the standard when it comes to dataframe keys, export column names to a csv. file.\n",
    "        \"\"\"\n",
    "        if self.foldertype==\"tuebingen\":\n",
    "            with open(path + '/covariate_names.csv', 'w', newline='') as csvfile:\n",
    "                writer = csv.writer(csvfile)\n",
    "                writer.writerow(full_df.keys())  # Write the dictionary keys as the header row\n",
    "\n",
    "                def write_dict_to_csv(d, parent_key=''):\n",
    "                    for key, value in d.items():\n",
    "                        if isinstance(value, dict):\n",
    "                            write_dict_to_csv(value)\n",
    "                        else:\n",
    "                            writer.writerow([parent_key + key] + list(value.columns))\n",
    "\n",
    "                write_dict_to_csv(full_df)\n",
    "            \n",
    "    def select_covariates(self, covariates: Dict = None):\n",
    "        \"\"\"\n",
    "        Select a subset of data based on the specified covariates of interest. Has to be specified by the user\n",
    "        \"\"\"\n",
    "        \n",
    "        #Set covaries of interest as given in the dictionary\n",
    "        if covariates is not None:\n",
    "            self.covariates = covariates\n",
    "        else:\n",
    "            #Manually select/compute the covariates of interest\n",
    "            self.covariates =  self.complete_data[\"mds_updrs\"] # Replace with the filtered data\n",
    "        self.is_converted_to_standard = True\n",
    "        return self.covariates\n",
    "\n",
    "    def to_longitudinal_data(self):\n",
    "        \"\"\"\n",
    "        Prepare the data for longitudinal analysis.\n",
    "        Return data in format:\n",
    "        output(dict) -> covariates(dict) -> PATNOs(dict) -> EXAMDATES(list)\n",
    "        \"\"\"\n",
    "        print(\"Preparing data for longitudinal analysis...\")\n",
    "        # Implement logic for preparing data for longitudinal analysis\n",
    "        data = self.covariates\n",
    "        self.df_longitude = {}\n",
    "        # Initialize OP_DATUM dictionary\n",
    "        self.df_longitude['OP_DATUM'] = {}\n",
    "        for key, value in data.items():\n",
    "            grouped_data = value.groupby('PATNO')\n",
    "            self.df_longitude[key] = {}\n",
    "            for patno, group in grouped_data:\n",
    "                # Convert 'TEST_DATUM' to string format\n",
    "                group['TEST_DATUM'] = pd.to_datetime(group['TEST_DATUM'], errors='coerce')\n",
    "                group['TEST_DATUM'] = group['TEST_DATUM'].apply(lambda x: x.strftime('%Y-%m-%d') if not pd.isnull(x) else x)\n",
    "                self.df_longitude[key][patno] = {'TEST_DATUM': group['TEST_DATUM'].tolist()}\n",
    "                \n",
    "                # Store OP_DATUM for each PATNO\n",
    "                if patno not in self.df_longitude['OP_DATUM']:\n",
    "                    # Convert 'OP_DATUM' to string format if it's not already a string\n",
    "                    if not isinstance(group['OP_DATUM'].iloc[0], str):\n",
    "                        group['OP_DATUM'] = pd.to_datetime(group['OP_DATUM'], errors='coerce')\n",
    "                        group['OP_DATUM'] = group['OP_DATUM'].apply(lambda x: x.strftime('%Y-%m-%d') if not pd.isnull(x) else x)\n",
    "                    self.df_longitude['OP_DATUM'][patno] = group['OP_DATUM'].iloc[0]   \n",
    "        return self.df_longitude    \n",
    "    #def to_df(self):\n",
    "    #    \"\"\"\n",
    "    #    Takes the already selected covariates and converts them to a pd df format\n",
    "    #    \"\"\"\n",
    "    #    # Merge all dataframes in the dictionary by 'PATNO'\n",
    "    #    merged_df = None\n",
    "    #    for key, df in self.covariates.items():\n",
    "    #        if merged_df is None:\n",
    "    #            merged_df = df\n",
    "    #        else:\n",
    "    #            merged_df = pd.merge(merged_df, df, on=['PATNO', 'TEST_DATUM'], how='outer')    \n",
    "    #            self.df = merged_df\n",
    "    #    return self.df\n",
    "\n",
    "    def convert_ppmi_to_standard_keys(self, file: str):\n",
    "        \"\"\"\n",
    "        Convert the PPMI data to the standard keys, which are based on the tuebingen data format.\n",
    "        \"\"\"\n",
    "        #Reading in the covariate names file\n",
    "        if self.is_converted_to_standard:\n",
    "            print(\"Already converted to standard\")\n",
    "            return\n",
    "        covariate_file = file\n",
    "        if not os.path.exists(covariate_file):\n",
    "            raise FileNotFoundError(f\"Covariate names file not found at {covariate_file}. Please provide the file.\")\n",
    "\n",
    "        covariate_dict = {}\n",
    "        with open(covariate_file, mode='r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            header = next(reader)\n",
    "            for rows in reader:\n",
    "                key = rows[0]\n",
    "                covariate_dict[key] = rows[1:]\n",
    "        self.complete_data\n",
    "\n",
    "        #Changing the keys of the dataframes to the standard keys\n",
    "        #Exchange keys of UPDRS\n",
    "        #UPDRS1\n",
    "        self.complete_data['mds_updrs']['mds_updrs1'].columns.values[6:12] = covariate_dict['mds_updrs1'][5:11]\n",
    "        self.complete_data['mds_updrs']['mds_updrs1'].columns.values[5:19] = covariate_dict['mds_updrs1'][4:18]\n",
    "\n",
    "        #UPDRS2\n",
    "        self.complete_data['mds_updrs']['mds_updrs2'].columns.values[6:20] = covariate_dict['mds_updrs2'][5:19]\n",
    "        self.complete_data['mds_updrs']['mds_updrs2'].rename(columns={'INFODT': covariate_dict['mds_updrs3'][4]}, inplace=True)  \n",
    "        #UPDRS3\n",
    "        #questions\n",
    "        self.complete_data['mds_updrs']['mds_updrs3'].columns.values[23:56] = covariate_dict['mds_updrs3'][8:41]\n",
    "        #HY scale\n",
    "        self.complete_data['mds_updrs']['mds_updrs3'].rename(columns={'NHY': covariate_dict['mds_updrs3'][-2]}, inplace=True)\n",
    "        #total score\n",
    "        self.complete_data['mds_updrs']['mds_updrs3'].rename(columns={'NP3TOT': covariate_dict['mds_updrs3'][-1]}, inplace=True)  \n",
    "        #Dyskinesia presence\n",
    "        self.complete_data['mds_updrs']['mds_updrs3'].rename(columns={'DYSKPRES': covariate_dict['mds_updrs3'][-4]}, inplace=True)  \n",
    "        #Exam date\n",
    "        self.complete_data['mds_updrs']['mds_updrs3'].rename(columns={'INFODT': covariate_dict['mds_updrs3'][4]}, inplace=True)  \n",
    "        #UPDRS4    \n",
    "        self.complete_data['mds_updrs']['mds_updrs4'].columns.values[[5, 9, 10, 14, 15, 16, 20]] = covariate_dict['mds_updrs4'][5:]\n",
    "        # Drop specified columns from UPDRS4\n",
    "        self.complete_data['mds_updrs']['mds_updrs4'].drop(self.complete_data['mds_updrs']['mds_updrs4'].columns[[6, 7, 8, 11, 12, 13, 17, 18, 19]], axis=1, inplace=True)\n",
    "\n",
    "        #Demographics\n",
    "        \n",
    "\n",
    "        #MOCA\n",
    "        moca_dict = {}\n",
    "        moca_dict['executive'] = self.complete_data['moca'].iloc[:, 5:9].sum(axis=1)\n",
    "        moca_dict['naming'] = self.complete_data['moca'].iloc[:, 10:12].sum(axis=1)\n",
    "        moca_dict['attention_numbers'] = self.complete_data['moca'].iloc[:, 13:14].sum(axis=1)\n",
    "        moca_dict['attention_letters'] = self.complete_data['moca'].iloc[:, 15]\n",
    "        moca_dict['attention_substract'] = self.complete_data['moca'].iloc[:, 16]\n",
    "        moca_dict['language_rep'] = self.complete_data['moca'].iloc[:, 17] \n",
    "        moca_dict['language_letters'] = self.complete_data['moca'].iloc[:, 18:19].sum(axis=1)\n",
    "        moca_dict['abstraction'] = self.complete_data['moca'].iloc[:, 20]\n",
    "        moca_dict['reminding'] = self.complete_data['moca'].iloc[:, 21:25].sum(axis=1)\n",
    "        moca_dict['orientation'] = self.complete_data['moca'].iloc[:, 26:31].sum(axis=1)\n",
    "        moca_dict['total'] = self.complete_data['moca'].iloc[:, 32]  \n",
    "        moca_dict = dict(zip(covariate_dict['moca'][5:16], moca_dict.values()))\n",
    "        self.complete_data['moca'] = pd.concat([\n",
    "            self.complete_data['moca'].iloc[:, :5],\n",
    "            pd.DataFrame(moca_dict),\n",
    "            self.complete_data['moca'].iloc[:, -2:]\n",
    "        ], axis=1)\n",
    "        self.complete_data['moca'].rename(columns={'INFODT': covariate_dict['moca'][4]}, inplace=True)  \n",
    "        self.is_converted_to_standard = True\n",
    "        return covariate_dict\n",
    "    \n",
    "    def convert_costom_to_standard_keys(self, file: str):\n",
    "        \"\"\"\n",
    "        convert the costom data to the standard keys, which are based on the tuebingen data format\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def remove_outliers(self):\n",
    "        \"\"\"\n",
    "        Placeholder for outlier removal. To be implemented.\n",
    "        \"\"\"\n",
    "        print(\"Removing outliers...\")\n",
    "        # Implement logic for removing outliers\n",
    "        pass\n",
    "\n",
    "\n",
    "# Propensity Matching Class\n",
    "class PropensityMatching:\n",
    "    def __init__(self, ppmi: Data, custom: Data):\n",
    "        \"\"\"\n",
    "        Initialize the propensity matching with PPMI and DBS data.\n",
    "        \"\"\"\n",
    "        self.ppmi = ppmi\n",
    "        self.custom = custom\n",
    "         \n",
    "        self.ppmi_for_model = None\n",
    "        self.custom_for_model = None\n",
    "\n",
    "    def match(self, matching_method: Callable, grouping_func: Callable, classification_model: Callable):\n",
    "        \"\"\"patno matching...\")\"\"\"\n",
    "        # Implement the matching logic using the provided methods\n",
    "        return None\n",
    "\n",
    "    def match_method1(self):\n",
    "        \"\"\"\n",
    "        Match on distance of preoperative test dates and follow up test dates.\n",
    "        \"\"\"\n",
    "        \n",
    "        def convert_dates_to_days(date_dict: Dict[str, Dict[str, list]]):\n",
    "            \"\"\"\n",
    "            Convert dates in the format 'YYYY-MM-DD' to the number of days since the first date.\n",
    "            \"\"\"\n",
    "            for covariate, sub_dict in date_dict.items():\n",
    "                for patno, date_list in sub_dict.items():\n",
    "                    # Convert string dates to datetime objects\n",
    "                    date_list = [datetime.strptime(date, '%Y-%m-%d') for date in date_list if date]\n",
    "                    if date_list:\n",
    "                        # Find the minimum date\n",
    "                        min_date = datetime(1950, 1, 1)    # Convert dates to the number of days since the minimum date\n",
    "                        date_dict[covariate][patno] = [(date - min_date).days for date in date_list]\n",
    "            return date_dict\n",
    "        \n",
    "        def find_preop_test(date_dict: Dict[str, Dict[str, list]]):\n",
    "            \"\"\"\n",
    "            Find the last test date preceding the operation for each patient in the DBS cohort.\n",
    "            \"\"\"\n",
    "            pass\n",
    "            \n",
    "        def match_on_preop():\n",
    "            \"\"\"\n",
    "            Match patients based on the preoperative test dates.\n",
    "            \"\"\"\n",
    "            pass\n",
    "\n",
    "        def match_on_followup():\n",
    "            \"\"\"\n",
    "            Match patients based on the follow-up dates.\n",
    "            \"\"\"\n",
    "            pass\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "    def grouping_method1(self):\n",
    "        \"\"\"\n",
    "        Example grouping method 1. Replace with actual logic.\n",
    "        \"\"\"\n",
    "        print(\"Using grouping method 1...\")\n",
    "        pass\n",
    "\n",
    "    def classification_model1(self):\n",
    "        \"\"\"\n",
    "        Example classification model 1. Replace with actual logic.\n",
    "        \"\"\"\n",
    "        print(\"Using classification model 1...\")\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_148994/3042578857.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  diaghist.SXDT = pd.to_datetime(diaghist.SXDT)\n"
     ]
    }
   ],
   "source": [
    "path_ppmi = '/home/georg-tirpitz/Documents/Neuromodulation/Parkinson_PSM/PPMI'\n",
    "path_tue = '/home/georg-tirpitz/Documents/Neuromodulation/QuestionnaireData'\n",
    "ppmi = Data(path_ppmi, foldertype=\"PPMI\")\n",
    "tue = Data(path_tue, foldertype=\"tuebingen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PATNO\n",
      "1 GEB_DATUM\n",
      "2 OP_DATUM\n",
      "3 OP_ZAHLEN\n",
      "4 TEST_DATUM\n",
      "5 MoCA_Executive\n",
      "6 MoCA_Benennen\n",
      "7 MoCA_Aufmerksamkeit_Zahlenliste\n",
      "8 MoCA_Aufmerksamkeit_Buchstabenliste\n",
      "9 MoCA_Aufmerksamkeit_Abziehen\n",
      "10 MoCA_Sprache_Wiederholen\n",
      "11 MoCA_Sprache_Buchstaben\n",
      "12 MoCA_Abstraktion\n",
      "13 MoCA_Erinnerung\n",
      "14 MoCA_Orientierung\n",
      "15 MoCA_ONLY_GES\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATNO</th>\n",
       "      <th>GEB_DATUM</th>\n",
       "      <th>OP_DATUM</th>\n",
       "      <th>OP_ZAHLEN</th>\n",
       "      <th>TEST_DATUM</th>\n",
       "      <th>MDS-UPDRS_1.1</th>\n",
       "      <th>MDS-UPDRS_1.2</th>\n",
       "      <th>MDS-UPDRS_1.3</th>\n",
       "      <th>MDS-UPDRS_1.4</th>\n",
       "      <th>MDS-UPDRS_1.5</th>\n",
       "      <th>MDS-UPDRS_1.6</th>\n",
       "      <th>MDS-UPDRS_1.7</th>\n",
       "      <th>MDS-UPDRS_1.8</th>\n",
       "      <th>MDS-UPDRS_1.9</th>\n",
       "      <th>MDS-UPDRS_1.10</th>\n",
       "      <th>MDS-UPDRS_1.11</th>\n",
       "      <th>MDS-UPDRS_1.12</th>\n",
       "      <th>MDS-UPDRS_1.13</th>\n",
       "      <th>MDS-UPDRS_1.TOT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526</td>\n",
       "      <td>1956-11-06 00:00:00</td>\n",
       "      <td>2018-04-06 00:00:00</td>\n",
       "      <td>2018-PD3</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>355</td>\n",
       "      <td>1968-02-20 00:00:00</td>\n",
       "      <td>2018-02-02 00:00:00</td>\n",
       "      <td>2018-PD1</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>551</td>\n",
       "      <td>1949-05-03 00:00:00</td>\n",
       "      <td>2018-03-12 00:00:00</td>\n",
       "      <td>2018-PD11</td>\n",
       "      <td>2017-11-13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>479</td>\n",
       "      <td>1945-01-12 00:00:00</td>\n",
       "      <td>2018-04-20 00:00:00</td>\n",
       "      <td>2018-PD5</td>\n",
       "      <td>2018-01-29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>543</td>\n",
       "      <td>1967-06-11 00:00:00</td>\n",
       "      <td>2018-04-13 00:00:00</td>\n",
       "      <td>2018-PD4</td>\n",
       "      <td>2018-02-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PATNO            GEB_DATUM             OP_DATUM  OP_ZAHLEN TEST_DATUM  \\\n",
       "0    526  1956-11-06 00:00:00  2018-04-06 00:00:00   2018-PD3 2017-07-14   \n",
       "1    355  1968-02-20 00:00:00  2018-02-02 00:00:00   2018-PD1 2017-10-16   \n",
       "2    551  1949-05-03 00:00:00  2018-03-12 00:00:00  2018-PD11 2017-11-13   \n",
       "3    479  1945-01-12 00:00:00  2018-04-20 00:00:00   2018-PD5 2018-01-29   \n",
       "4    543  1967-06-11 00:00:00  2018-04-13 00:00:00   2018-PD4 2018-02-27   \n",
       "\n",
       "   MDS-UPDRS_1.1  MDS-UPDRS_1.2  MDS-UPDRS_1.3  MDS-UPDRS_1.4  MDS-UPDRS_1.5  \\\n",
       "0            0.0            0.0            0.0            0.0            0.0   \n",
       "1            0.0            0.0            1.0            0.0            0.0   \n",
       "2            0.0            0.0            0.0            0.0            0.0   \n",
       "3            0.0            0.0            0.0            0.0            0.0   \n",
       "4            0.0            0.0            0.0            0.0            0.0   \n",
       "\n",
       "   MDS-UPDRS_1.6  MDS-UPDRS_1.7  MDS-UPDRS_1.8  MDS-UPDRS_1.9  MDS-UPDRS_1.10  \\\n",
       "0            0.0            3.0            0.0            0.0             0.0   \n",
       "1            0.0            1.0            1.0            1.0             1.0   \n",
       "2            0.0            3.0            0.0            0.0             1.0   \n",
       "3            0.0            3.0            2.0            2.0             2.0   \n",
       "4            0.0            2.0            0.0            1.0             1.0   \n",
       "\n",
       "   MDS-UPDRS_1.11  MDS-UPDRS_1.12  MDS-UPDRS_1.13  MDS-UPDRS_1.TOT  \n",
       "0             1.0             0.0             0.0              4.0  \n",
       "1             0.0             0.0             0.0              5.0  \n",
       "2             0.0             0.0             0.0              4.0  \n",
       "3             1.0             0.0             1.0             11.0  \n",
       "4             0.0             0.0             0.0              4.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ic(dbs.complete_data['demo'].keys())\n",
    "#tue.complete_data['moca'].head(5)\n",
    "keys = tue.complete_data['moca'].keys()\n",
    "for i, key in enumerate(keys):\n",
    "    print(i, key)\n",
    "# Find and print two instances where the PATNO is the same\n",
    "tue.complete_data[\"mds_updrs\"]['mds_updrs1'].head(5)\n",
    "#ic(type(dbs.complete_data['demo']['Unique identifier']))\n",
    "#ic(ppmi.complete_data['updrs'].head(1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 REC_ID\n",
      "1 PATNO\n",
      "2 EVENT_ID\n",
      "3 PAG_NAME\n",
      "4 INFODT\n",
      "5 NUPSOURC\n",
      "6 NP2SPCH\n",
      "7 NP2SALV\n",
      "8 NP2SWAL\n",
      "9 NP2EAT\n",
      "10 NP2DRES\n",
      "11 NP2HYGN\n",
      "12 NP2HWRT\n",
      "13 NP2HOBB\n",
      "14 NP2TURN\n",
      "15 NP2TRMR\n",
      "16 NP2RISE\n",
      "17 NP2WALK\n",
      "18 NP2FREZ\n",
      "19 NP2PTOT\n",
      "20 ORIG_ENTRY\n",
      "21 LAST_UPDATE\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "keys = ppmi.complete_data[\"mds_updrs\"][\"mds_updrs2\"].keys()\n",
    "for i, key in enumerate(keys):\n",
    "    print(i, key)\n",
    "#['NP4WDYSK','NP4DYSKI','NP4OFF','NP4FLCTI','NP4FLCTX','NP4DYSTN', 'NP4TOT'] 5, 9, 10, 14, 15, 16, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REC_ID', 'PATNO', 'EVENT_ID', 'PAG_NAME', 'TEST_DATUM',\n",
       "       'MoCA_Executive', 'MoCA_Benennen', 'MoCA_Aufmerksamkeit_Zahlenliste',\n",
       "       'MoCA_Aufmerksamkeit_Buchstabenliste', 'MoCA_Aufmerksamkeit_Abziehen',\n",
       "       'MoCA_Sprache_Wiederholen', 'MoCA_Sprache_Buchstaben',\n",
       "       'MoCA_Abstraktion', 'MoCA_Erinnerung', 'MoCA_Orientierung',\n",
       "       'MoCA_ONLY_GES', 'ORIG_ENTRY', 'LAST_UPDATE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates = ppmi.convert_ppmi_to_standard_keys('/home/georg-tirpitz/Documents/Neuromodulation/QuestionnaireData/covariate_names.csv')\n",
    "ppmi.complete_data['moca'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for longitudinal analysis...\n",
      "Preparing data for longitudinal analysis...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{206: '2018-02-23',\n",
       " 228: '2019-04-05',\n",
       " 240: '2019-08-09',\n",
       " 258: '2020-07-03',\n",
       " 295: '2022-03-23',\n",
       " 298: '2022-05-06',\n",
       " 302: '2022-06-03',\n",
       " 303: '2022-06-10',\n",
       " 317: '2022-11-18',\n",
       " 318: '2022-11-25',\n",
       " 349: '2024-07-26',\n",
       " 350: '2024-08-02',\n",
       " 355: '2018-02-02',\n",
       " 360: '2018-12-14',\n",
       " 364: '2019-07-05',\n",
       " 366: '2019-06-07',\n",
       " 367: '2019-07-26',\n",
       " 368: '2019-08-09',\n",
       " 370: '2019-07-19',\n",
       " 371: '2019-10-18',\n",
       " 372: '2019-11-30',\n",
       " 375: '2019-10-25',\n",
       " 377: '2020-06-12',\n",
       " 378: '2020-01-17',\n",
       " 381: '2020-06-05',\n",
       " 382: '2020-06-19',\n",
       " 384: '2021-11-12',\n",
       " 385: '2020-05-29',\n",
       " 386: '2020-08-21',\n",
       " 387: '2020-09-25',\n",
       " 388: '2020-07-10',\n",
       " 389: '2020-09-11',\n",
       " 390: '2021-02-12',\n",
       " 391: '2020-11-13',\n",
       " 392: '2020-10-30',\n",
       " 393: '2020-11-20',\n",
       " 394: '2021-01-29',\n",
       " 396: '2022-06-17',\n",
       " 397: '2021-02-19',\n",
       " 401: '2021-05-21',\n",
       " 402: '2021-07-02',\n",
       " 403: '2021-04-16',\n",
       " 404: '2021-05-07',\n",
       " 406: '2021-04-09',\n",
       " 411: '2021-10-22',\n",
       " 413: '2021-11-05',\n",
       " 417: '2022-03-23',\n",
       " 420: '2022-05-06',\n",
       " 422: '2022-09-23',\n",
       " 423: '2022-06-24',\n",
       " 425: '2022-07-15',\n",
       " 428: '2022-06-10',\n",
       " 429: '2022-07-29',\n",
       " 431: '2022-12-09',\n",
       " 433: '2022-11-25',\n",
       " 435: '2023-02-10',\n",
       " 436: '2022-11-18',\n",
       " 441: '2023-03-10',\n",
       " 443: '2022-03-04',\n",
       " 444: '2022-03-04',\n",
       " 445: '2022-10-28',\n",
       " 446: '2023-06-02',\n",
       " 447: '2022-09-23',\n",
       " 448: '2021-07-16',\n",
       " 449: '2021-02-05',\n",
       " 450: '2020-10-23',\n",
       " 451: '2016-02-05',\n",
       " 452: '2022-11-04',\n",
       " 455: '2023-06-30',\n",
       " 456: '2022-09-30',\n",
       " 457: '2019-01-18',\n",
       " 458: '2023-09-15',\n",
       " 459: '2021-10-22',\n",
       " 460: '2018-08-10',\n",
       " 462: '2020-07-31',\n",
       " 463: '2021-11-26',\n",
       " 465: '2020-09-04',\n",
       " 466: '2016-07-22',\n",
       " 468: '2014-01-17',\n",
       " 469: '2023-02-03',\n",
       " 471: '2014-01-27',\n",
       " 473: '2021-04-23',\n",
       " 474: '2022-05-20',\n",
       " 475: '2019-04-05',\n",
       " 476: '2020-05-22',\n",
       " 478: '2019-11-15',\n",
       " 479: '2018-04-20',\n",
       " 480: '2022-08-05',\n",
       " 482: '2007-09-10',\n",
       " 483: '2009-06-05',\n",
       " 486: '2021-11-19',\n",
       " 487: '2017-06-09',\n",
       " 488: '2017-04-21',\n",
       " 489: '2022-07-22',\n",
       " 491: '2023-01-20',\n",
       " 493: '2017-03-17',\n",
       " 494: '2022-12-16',\n",
       " 495: '2015-09-11',\n",
       " 496: '2017-03-21',\n",
       " 497: '2016-12-09',\n",
       " 498: '2014-06-06',\n",
       " 499: '2013-01-18',\n",
       " 500: '2018-05-18',\n",
       " 501: '2017-07-07',\n",
       " 502: '2014-10-21',\n",
       " 503: '2022-12-09',\n",
       " 504: '2014-05-19',\n",
       " 505: '2021-11-26',\n",
       " 506: '2023-03-10',\n",
       " 507: '2020-05-29',\n",
       " 508: '2022-04-01',\n",
       " 509: '2022-07-29',\n",
       " 510: '2018-07-06',\n",
       " 511: '2016-09-09',\n",
       " 512: '2012-01-20',\n",
       " 513: '2019-02-01',\n",
       " 514: '2019-02-01',\n",
       " 515: '2015-10-12',\n",
       " 516: '2018-11-23',\n",
       " 517: '2019-08-02',\n",
       " 518: '2017-01-27',\n",
       " 519: '2024-04-12',\n",
       " 520: '2019-12-06',\n",
       " 521: '2023-08-11',\n",
       " 522: '2019-07-26',\n",
       " 523: '2019-05-10',\n",
       " 524: '2022-05-27',\n",
       " 525: '2023-01-27',\n",
       " 526: '2018-04-06',\n",
       " 527: '2021-08-13',\n",
       " 528: '2023-08-25',\n",
       " 529: '2024-05-02',\n",
       " 530: '2021-03-19',\n",
       " 531: '2017-09-01',\n",
       " 532: '2019-04-08',\n",
       " 533: '2016-05-13',\n",
       " 534: '2019-05-06',\n",
       " 535: '2022-07-01',\n",
       " 536: '2018-10-26',\n",
       " 537: '2018-07-13',\n",
       " 538: '2022-09-16',\n",
       " 539: '2007-05-07',\n",
       " 540: '2019-05-24',\n",
       " 541: '2019-09-13',\n",
       " 542: '2021-07-30',\n",
       " 543: '2018-04-13',\n",
       " 544: '2023-05-05',\n",
       " 545: '2023-12-18',\n",
       " 546: '2024-06-28',\n",
       " 547: '2022-06-03',\n",
       " 548: '2015-08-28',\n",
       " 549: '2019-09-20',\n",
       " 550: '2021-07-10',\n",
       " 551: '2018-03-12',\n",
       " 552: '2024-07-12',\n",
       " 553: '2011-07-16',\n",
       " 554: '2022-07-08',\n",
       " 555: '2021-10-08',\n",
       " 556: '2024-07-19',\n",
       " 557: '2022-07-27',\n",
       " 558: '2019-05-17',\n",
       " 559: '2021-09-03',\n",
       " 560: '2022-05-13',\n",
       " 561: '2019-11-08',\n",
       " 562: '2022-04-01',\n",
       " 563: '2008-04-18',\n",
       " 564: '2020-10-09',\n",
       " 565: '2020-07-17',\n",
       " 566: '2024-01-09',\n",
       " 567: '2013-06-14',\n",
       " 568: '2020-08-28',\n",
       " 569: '2017-10-13',\n",
       " 570: '2024-03-22',\n",
       " 571: '2018-05-18',\n",
       " 572: '2022-04-08',\n",
       " 573: '2021-05-14',\n",
       " 574: '2022-09-26',\n",
       " 575: '2007-07-16',\n",
       " 576: '2011-07-11',\n",
       " 577: '2022-09-26',\n",
       " 4: '2007-05-07',\n",
       " 11: '2007-09-03',\n",
       " 12: '2007-09-10',\n",
       " 15: '2007-11-12',\n",
       " 17: '2007-11-21',\n",
       " 22: '2008-04-18',\n",
       " 24: '2008-08-08',\n",
       " 26: '2008-08-25',\n",
       " 29: '2008-10-27',\n",
       " 32: '2009-01-26',\n",
       " 33: '2009-02-16',\n",
       " 40: '2009-06-22',\n",
       " 44: '2009-08-03',\n",
       " 45: '2009-08-17',\n",
       " 47: '2009-08-28',\n",
       " 50: '2009-09-21',\n",
       " 51: '2009-10-12',\n",
       " 52: '2009-10-23',\n",
       " 53: '2009-10-26',\n",
       " 58: '2009-12-18',\n",
       " 65: '2010-11-15',\n",
       " 71: '2011-03-14',\n",
       " 74: '2011-05-30',\n",
       " 78: '2011-10-31',\n",
       " 81: '2011-11-21',\n",
       " 83: '2012-01-20',\n",
       " 85: '2012-02-20',\n",
       " 88: '2012-04-16',\n",
       " 90: '2012-06-25',\n",
       " 91: '2012-06-29',\n",
       " 99: '2013-01-18',\n",
       " 100: '2013-01-25',\n",
       " 102: '2013-02-08',\n",
       " 109: '2013-06-28',\n",
       " 111: '2013-07-19',\n",
       " 112: '2013-07-26',\n",
       " 114: '2013-08-09',\n",
       " 119: '2013-12-06',\n",
       " 120: '2014-01-17',\n",
       " 123: '2014-04-07',\n",
       " 126: '2014-04-25',\n",
       " 130: '2014-05-09',\n",
       " 133: '2014-06-06',\n",
       " 135: '2014-07-14',\n",
       " 136: '2014-08-01',\n",
       " 137: '2014-08-04',\n",
       " 138: '2014-08-08',\n",
       " 139: '2014-08-11',\n",
       " 140: '2014-08-15',\n",
       " 142: '2014-09-08',\n",
       " 146: '2014-10-21',\n",
       " 152: '2015-04-24',\n",
       " 158: '2015-10-05',\n",
       " 159: '2015-10-12',\n",
       " 162: '2015-11-20',\n",
       " 165: '2016-01-11',\n",
       " 167: '2016-02-05',\n",
       " 169: '2016-05-13',\n",
       " 174: '2016-07-01',\n",
       " 176: '2016-07-22',\n",
       " 179: '2016-09-09',\n",
       " 181: '2016-10-07',\n",
       " 182: '2016-10-10',\n",
       " 183: '2016-10-17',\n",
       " 184: '2016-10-28',\n",
       " 185: '2016-11-18',\n",
       " 186: '2016-11-21',\n",
       " 187: '2016-11-28',\n",
       " 188: '2016-12-09',\n",
       " 191: '2017-03-03',\n",
       " 192: '2017-03-17',\n",
       " 193: '2017-03-21',\n",
       " 194: '2017-04-07',\n",
       " 196: '2017-04-21',\n",
       " 197: '2017-05-12',\n",
       " 198: '2017-06-09',\n",
       " 200: '2017-07-07',\n",
       " 202: '2017-10-13',\n",
       " 203: '2017-11-17',\n",
       " 204: '2017-11-24',\n",
       " 205: '2018-02-02',\n",
       " 207: '2018-03-09',\n",
       " 208: '2018-03-12',\n",
       " 209: '2018-04-06',\n",
       " 210: '2018-04-13',\n",
       " 211: '2018-04-20',\n",
       " 212: '2018-05-18',\n",
       " 213: '2018-06-08',\n",
       " 214: '2018-07-06',\n",
       " 215: '2018-07-13',\n",
       " 216: '2018-08-03',\n",
       " 217: '2018-08-10',\n",
       " 218: '2018-10-26',\n",
       " 219: '2018-11-09',\n",
       " 220: '2018-11-23',\n",
       " 221: '2018-12-14',\n",
       " 222: '2019-01-18',\n",
       " 223: '2019-02-01',\n",
       " 224: '2019-03-01',\n",
       " 225: '2019-03-04',\n",
       " 226: '2019-03-15',\n",
       " 227: '2019-03-25',\n",
       " 229: '2019-04-08',\n",
       " 230: '2019-05-06',\n",
       " 231: '2019-05-10',\n",
       " 232: '2019-05-17',\n",
       " 233: '2019-05-24',\n",
       " 234: '2019-06-07',\n",
       " 235: '2019-07-05',\n",
       " 236: '2019-07-12',\n",
       " 237: '2019-07-19',\n",
       " 239: '2019-08-02',\n",
       " 241: '2019-08-23',\n",
       " 242: '2019-09-13',\n",
       " 243: '2019-09-20',\n",
       " 244: '2019-10-11',\n",
       " 245: '2019-10-18',\n",
       " 246: '2019-10-25',\n",
       " 247: '2019-11-08',\n",
       " 248: '2019-11-15',\n",
       " 249: '2019-11-30',\n",
       " 250: '2019-12-06',\n",
       " 251: '2020-01-17',\n",
       " 252: '2020-05-22',\n",
       " 253: '2020-05-29',\n",
       " 254: '2020-06-05',\n",
       " 255: '2020-06-12',\n",
       " 256: '2020-06-19',\n",
       " 257: '2020-06-26',\n",
       " 259: '2020-07-10',\n",
       " 260: '2020-07-17',\n",
       " 261: '2020-07-31',\n",
       " 262: '2020-08-21',\n",
       " 263: '2020-08-28',\n",
       " 264: '2020-09-04',\n",
       " 265: '2020-09-11',\n",
       " 266: '2020-09-25',\n",
       " 267: '2020-10-09',\n",
       " 268: '2020-10-23',\n",
       " 269: '2020-10-30',\n",
       " 270: '2020-11-13',\n",
       " 271: '2020-11-20',\n",
       " 272: '2021-01-15',\n",
       " 273: '2021-01-29',\n",
       " 274: '2021-02-05',\n",
       " 275: '2021-02-12',\n",
       " 276: '2021-02-19',\n",
       " 277: '2021-03-19',\n",
       " 278: '2021-04-09',\n",
       " 279: '2021-04-16',\n",
       " 280: '2021-04-23',\n",
       " 281: '2021-05-14',\n",
       " 282: '2021-05-21',\n",
       " 283: '2021-07-02',\n",
       " 284: '2021-07-16',\n",
       " 285: '2021-07-30',\n",
       " 286: '2021-08-13',\n",
       " 287: '2021-09-03',\n",
       " 288: '2021-10-08',\n",
       " 289: '2021-10-22',\n",
       " 290: '2021-11-05',\n",
       " 291: '2021-11-12',\n",
       " 292: '2021-11-19',\n",
       " 293: '2021-11-26',\n",
       " 294: '2022-03-04',\n",
       " 297: '2022-04-08',\n",
       " 299: '2022-05-13',\n",
       " 300: '2022-05-20',\n",
       " 301: '2022-05-27',\n",
       " 304: '2022-06-17',\n",
       " 305: '2022-06-24',\n",
       " 306: '2022-07-01',\n",
       " 307: '2022-07-08',\n",
       " 308: '2022-07-15',\n",
       " 309: '2022-07-22',\n",
       " 310: '2022-07-29',\n",
       " 311: '2022-08-05',\n",
       " 312: '2022-09-16',\n",
       " 313: '2022-09-23',\n",
       " 314: '2022-09-30',\n",
       " 315: '2022-10-28',\n",
       " 316: '2022-11-04',\n",
       " 319: '2022-12-09',\n",
       " 320: '2022-12-16',\n",
       " 321: '2023-01-20',\n",
       " 322: '2023-01-27',\n",
       " 323: '2023-02-03',\n",
       " 324: '2023-02-10',\n",
       " 325: '2023-03-10',\n",
       " 351: '2024-08-09',\n",
       " 578: '2022-06-07',\n",
       " 579: '2013-06-28',\n",
       " 580: '2013-06-28',\n",
       " 582: '2015-12-04',\n",
       " 583: '2015-04-10',\n",
       " 584: '2022-06-05',\n",
       " 587: '2017-06-12',\n",
       " 588: '2022-06-09',\n",
       " 589: '2017-03-21',\n",
       " 591: '2017-04-10',\n",
       " 594: '2017-05-12',\n",
       " 598: '2019-07-26',\n",
       " 600: '2020-06-12',\n",
       " 601: '2022-06-12',\n",
       " 603: '2021-05-07',\n",
       " 604: '2022-06-08',\n",
       " 605: '2022-06-13',\n",
       " 606: '2022-06-11',\n",
       " 607: '2022-06-04',\n",
       " 608: '2022-06-06',\n",
       " 609: '2022-06-10',\n",
       " 610: '2022-06-14',\n",
       " 611: '2020-07-30',\n",
       " 612: '2018-11-09',\n",
       " 613: '2021-01-15',\n",
       " 614: '2024-06-23'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tue.select_covariates()\n",
    "#ppmi.select_covariates()\n",
    "#print(tue.df)\n",
    "tue.to_longitudinal_data()['mds_updrs3']\n",
    "tue.to_longitudinal_data()['OP_DATUM']\n",
    "#ppmi.to_longitudinal_data()\n",
    "#print(type(long['mds_updrs3'][610]['TEST_DATUM'][0]))\n",
    "#print(type(long['mds_updrs3'][4]['TEST_DATUM'][0]))\n",
    "#type(long['mds_updrs3'][4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
